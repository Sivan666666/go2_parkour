import torch
import torch.nn as nn
import sys
import torchvision
from transformers import ViTModel, ViTConfig

class RecurrentDepthBackbone(nn.Module):
    def __init__(self, base_backbone, env_cfg) -> None:
        super().__init__()
        activation = nn.ELU()
        last_activation = nn.Tanh()
        self.base_backbone = base_backbone
        
        if env_cfg == None:
            # åŠ æ·±ç½‘ç»œæ·±åº¦ï¼š3å±‚ MLP
            self.combination_mlp = nn.Sequential(
                                    nn.Linear(32 + 53, 256),
                                    activation,
                                    nn.Linear(256, 128),
                                    activation,
                                    nn.Linear(128, 64)
                                )
        else:
            # åŠ æ·±ç½‘ç»œæ·±åº¦ï¼š3å±‚ MLP
            self.combination_mlp = nn.Sequential(
                                        nn.Linear(32 + env_cfg.env.n_proprio, 256),
                                        activation,
                                        nn.Linear(256, 128),
                                        activation,
                                        nn.Linear(128, 64)
                                    )
        
        # åŠ æ·± GRUï¼š2å±‚ï¼Œå¢åŠ éšè—å±‚ç»´åº¦åˆ° 768
        self.rnn = nn.GRU(
            input_size=64, 
            hidden_size=768, 
            num_layers=2,      # ğŸ”¥ ä» 1 å±‚å¢åŠ åˆ° 2 å±‚
            batch_first=True,
            dropout=0.1        # æ·»åŠ  dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        
        # åŠ æ·±è¾“å‡º MLPï¼š3å±‚
        self.output_mlp = nn.Sequential(
                                nn.Linear(768, 256),
                                activation,
                                nn.Linear(256, 128),
                                activation,
                                nn.Linear(128, 32+2),
                                last_activation
                            )
        self.hidden_states = None

    def forward(self, depth_image, proprioception):
        depth_image = self.base_backbone(depth_image)  # [batch, 32]
        depth_latent = self.combination_mlp(torch.cat((depth_image, proprioception), dim=-1))  # [batch, 64]
        
        # RNN å¤„ç†
        depth_latent, self.hidden_states = self.rnn(depth_latent[:, None, :], self.hidden_states)  # [batch, 1, 768]
        depth_latent = self.output_mlp(depth_latent.squeeze(1))  # [batch, 34]
        
        return depth_latent

    def detach_hidden_states(self):
        if self.hidden_states is not None:
            self.hidden_states = self.hidden_states.detach().clone()


import torch
import torch.nn as nn
import math

class DepthTransformerBackbone(nn.Module):
    """è½»é‡çº§å•é€šé“æ·±åº¦å›¾ Transformerï¼ˆæ˜¾å­˜å‹å¥½ï¼‰"""
    
    def __init__(self, prop_dim, scandots_output_dim, hidden_state_dim, output_activation=None, num_frames=1):
        super().__init__()
        
        self.scandots_output_dim = scandots_output_dim
        activation = nn.ELU()
        
        # ==========================================
        # ğŸ”¥ è½»é‡çº§è¶…å‚æ•°ï¼ˆå¤§å¹…å‡å°‘å‚æ•°é‡ï¼‰
        # ==========================================
        self.patch_size = 16         # ğŸ”¥ ä» 8 å¢åŠ åˆ° 16 (å‡å°‘ patch æ•°é‡)
        self.embed_dim = 128         # ğŸ”¥ ä» 256 å‡å°‘åˆ° 128 (å‡å°‘éšè—ç»´åº¦)
        self.num_heads = 4           # ğŸ”¥ ä» 8 å‡å°‘åˆ° 4 (å‡å°‘æ³¨æ„åŠ›å¤´)
        self.num_layers = 3          # ğŸ”¥ ä» 6 å‡å°‘åˆ° 3 (å‡å°‘å±‚æ•°)
        self.mlp_ratio = 2           # ğŸ”¥ ä» 4 å‡å°‘åˆ° 2 (FFN æ›´å°)
        
        # è®¡ç®— patch æ•°é‡
        self.num_patches_h = 58 // self.patch_size  # 3
        self.num_patches_w = 87 // self.patch_size  # 5
        self.num_patches = self.num_patches_h * self.num_patches_w  # 15 (åŸæ¥ 70)
        
        # ==========================================
        # 1. Patch Embeddingï¼ˆå•é€šé“å·ç§¯ï¼‰
        # ==========================================
        self.patch_embed = nn.Conv2d(
            in_channels=1,
            out_channels=self.embed_dim,
            kernel_size=self.patch_size,
            stride=self.patch_size,
            padding=0
        )
        
        # ==========================================
        # 2. CLS Token å’Œ Position Embedding
        # ==========================================
        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.embed_dim))
        
        # åˆå§‹åŒ–
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        
        # ==========================================
        # 3. Transformer Encoderï¼ˆæ›´è½»é‡ï¼‰
        # ==========================================
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.embed_dim,
            nhead=self.num_heads,
            dim_feedforward=self.embed_dim * self.mlp_ratio,  # ğŸ”¥ 256 (åŸæ¥ 1024)
            dropout=0.1,
            activation='gelu',
            batch_first=True,
            norm_first=True  # ğŸ”¥ Pre-LN æ›´ç¨³å®šï¼Œæ”¶æ•›æ›´å¿«
        )
        
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=self.num_layers
        )
        
        # ==========================================
        # 4. è¾“å‡º MLPï¼ˆæ›´ç®€å•ï¼‰
        # ==========================================
        self.output_mlp = nn.Sequential(
            nn.LayerNorm(self.embed_dim),
            nn.Linear(self.embed_dim, 128),  # ğŸ”¥ ä» 256 å‡å°‘åˆ° 128
            activation,
            nn.Dropout(0.1),
            nn.Linear(128, scandots_output_dim)
        )
        
        if output_activation == "tanh":
            self.output_activation = nn.Tanh()
        else:
            self.output_activation = activation
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        
        print(f"âœ… Lightweight Depth Transformer initialized:")
        print(f"   - Input size: [58, 87]")
        print(f"   - Patch size: {self.patch_size}x{self.patch_size}")
        print(f"   - Num patches: {self.num_patches} (å‡å°‘ {100 - self.num_patches/70*100:.1f}%)")
        print(f"   - Embed dim: {self.embed_dim} (å‡å°‘ 50%)")
        print(f"   - Num layers: {self.num_layers} (å‡å°‘ 50%)")
        print(f"   - Num heads: {self.num_heads} (å‡å°‘ 50%)")
        print(f"   - FFN hidden: {self.embed_dim * self.mlp_ratio} (å‡å°‘ 75%)")
        print(f"   - Output dim: {scandots_output_dim}")
        print(f"   - Total parameters: {total_params:,} (~{total_params/1e6:.2f}M)")
        print(f"   - ä¼°è®¡æ˜¾å­˜å ç”¨: ~{total_params * 4 / 1024**2 * 2:.1f} MB (FP32)")

    def forward(self, images: torch.Tensor):
        """
        è¾“å…¥: images [batch, 58, 87] æ·±åº¦å›¾
        è¾“å‡º: latent [batch, scandots_output_dim] (32 ç»´)
        """
        batch_size = images.shape[0]
        
        # ==========================================
        # 1. Patch Embedding
        # ==========================================
        x = images.unsqueeze(1)  # [batch, 1, 58, 87]
        x = self.patch_embed(x)  # [batch, 128, 3, 5]
        x = x.flatten(2).transpose(1, 2)  # [batch, 15, 128]
        
        # ==========================================
        # 2. æ·»åŠ  CLS Token
        # ==========================================
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # [batch, 1, 128]
        x = torch.cat([cls_tokens, x], dim=1)  # [batch, 16, 128]
        
        # ==========================================
        # 3. æ·»åŠ  Position Embedding
        # ==========================================
        x = x + self.pos_embed  # [batch, 16, 128]
        
        # ==========================================
        # 4. Transformer Encoder
        # ==========================================
        x = self.transformer(x)  # [batch, 16, 128]
        
        # ==========================================
        # 5. å– CLS Token ä½œä¸ºå…¨å±€ç‰¹å¾
        # ==========================================
        cls_output = x[:, 0]  # [batch, 128]
        
        # ==========================================
        # 6. MLP æ˜ å°„åˆ°ç›®æ ‡ç»´åº¦
        # ==========================================
        latent = self.output_mlp(cls_output)  # [batch, 32]
        latent = self.output_activation(latent)
        
        return latent


# ä½¿ç”¨åˆ«åä¿æŒå…¼å®¹æ€§
DepthOnlyFCBackbone58x87 = DepthTransformerBackbone