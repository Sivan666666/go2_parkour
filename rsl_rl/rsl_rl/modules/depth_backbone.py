import torch
import torch.nn as nn
import sys
import torchvision
from transformers import ViTModel, ViTConfig

# ==========================================
# ğŸ”¥ æ–°ç‰ˆæœ¬ 1: ResNet Backbone
# ==========================================
class DepthResNetBackbone(nn.Module):
    """
    ä½¿ç”¨ ResNet18 æ›¿ä»£ Transformer è¿›è¡Œæ·±åº¦å›¾ç‰¹å¾æå–
    æ›´è½»é‡ã€æ›´å¿«ã€æ›´ç¨³å®š
    """
    def __init__(self, prop_dim, scandots_output_dim, hidden_state_dim, output_activation=None, num_frames=1):
        super().__init__()
        
        self.scandots_output_dim = scandots_output_dim
        activation = nn.ELU()
        
        # ==========================================
        # 1. ä½¿ç”¨é¢„è®­ç»ƒ ResNet18ï¼ˆå»æ‰ FC å±‚ï¼‰
        # ==========================================
        resnet = torchvision.models.resnet18(pretrained=False)
        
        # ä¿®æ”¹ç¬¬ä¸€å±‚å·ç§¯ï¼ˆå•é€šé“è¾“å…¥ï¼‰
        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')
        
        # å¤ç”¨ ResNet çš„å…¶ä»–å±‚
        self.bn1 = resnet.bn1
        self.relu = resnet.relu
        self.maxpool = resnet.maxpool
        self.layer1 = resnet.layer1  # 64 channels
        self.layer2 = resnet.layer2  # 128 channels
        self.layer3 = resnet.layer3  # 256 channels
        self.layer4 = resnet.layer4  # 512 channels
        
        # ==========================================
        # 2. è‡ªé€‚åº”å¹³å‡æ± åŒ–ï¼ˆè¾“å‡ºå›ºå®šå¤§å°ï¼‰
        # ==========================================
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        # ==========================================
        # 3. è¾“å‡º MLP
        # ==========================================
        self.output_mlp = nn.Sequential(
            nn.Linear(512, 256),
            activation,
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            activation,
            nn.Dropout(0.1),
            nn.Linear(128, scandots_output_dim)
        )
        
        if output_activation == "tanh":
            self.output_activation = nn.Tanh()
        else:
            self.output_activation = activation
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        
        print(f"âœ… ResNet18 Depth Backbone initialized:")
        print(f"   - Input size: [58, 87]")
        print(f"   - Architecture: ResNet18 (modified for single channel)")
        print(f"   - Output dim: {scandots_output_dim}")
        print(f"   - Total parameters: {total_params:,} (~{total_params/1e6:.2f}M)")
        print(f"   - ä¼°è®¡æ˜¾å­˜å ç”¨: ~{total_params * 4 / 1024**2 * 2:.1f} MB (FP32)")

    def forward(self, images: torch.Tensor):
        """
        è¾“å…¥: images [batch, 58, 87] æ·±åº¦å›¾
        è¾“å‡º: latent [batch, scandots_output_dim] (32 ç»´)
        """
        # ==========================================
        # 1. ResNet å‰å‘ä¼ æ’­
        # ==========================================
        x = images.unsqueeze(1)  # [batch, 1, 58, 87]
        
        # Conv1 + BN + ReLU + MaxPool
        x = self.conv1(x)        # [batch, 64, 29, 44]
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)      # [batch, 64, 15, 22]
        
        # ResNet Blocks
        x = self.layer1(x)       # [batch, 64, 15, 22]
        x = self.layer2(x)       # [batch, 128, 8, 11]
        x = self.layer3(x)       # [batch, 256, 4, 6]
        x = self.layer4(x)       # [batch, 512, 2, 3]
        
        # ==========================================
        # 2. å…¨å±€å¹³å‡æ± åŒ–
        # ==========================================
        x = self.avgpool(x)      # [batch, 512, 1, 1]
        x = torch.flatten(x, 1)  # [batch, 512]
        
        # ==========================================
        # 3. MLP æ˜ å°„åˆ°ç›®æ ‡ç»´åº¦
        # ==========================================
        latent = self.output_mlp(x)  # [batch, 32]
        latent = self.output_activation(latent)
        
        return latent

class RecurrentDepthBackbone_Attention(nn.Module):
    """
    ä½¿ç”¨ 3 å±‚ Self-Attention æ›¿ä»£ MLP è¿›è¡Œç‰¹å¾èåˆ
    Proprioception å…ˆç»è¿‡ MLP ç¼–ç ï¼Œå†ä¸ Depth ä¸€èµ·é€å…¥å¤šå±‚ Attention
    """
    def __init__(self, base_backbone, env_cfg) -> None:
        super().__init__()
        activation = nn.ELU()
        last_activation = nn.Tanh()
        self.base_backbone = base_backbone
        
        # ==========================================
        # 1. Proprioception MLP ç¼–ç å™¨
        # ==========================================
        if env_cfg == None:
            proprio_dim = 53
        else:
            proprio_dim = env_cfg.env.n_proprio
        
        self.proprio_encoder = nn.Sequential(
            nn.Linear(proprio_dim, 128),
            activation,
            nn.Linear(128, 64),
            activation,
            nn.Linear(64, 32)  # è¾“å‡ºä¸ depth_latent ç›¸åŒç»´åº¦
        )
        
        # ==========================================
        # 2. ğŸ”¥ 3 å±‚ Self-Attentionï¼ˆTransformer Encoderï¼‰
        # ==========================================
        self.embed_dim = 32  # depth å’Œ proprio ç¼–ç åçš„ç»´åº¦
        self.num_heads = 4
        self.num_attn_layers = 3  # ğŸ”¥ ä» 1 å±‚å¢åŠ åˆ° 3 å±‚
        
        # åˆ›å»º Transformer Encoderï¼ˆ3 å±‚ï¼‰
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.embed_dim,
            nhead=self.num_heads,
            dim_feedforward=128,  # FFN éšè—å±‚ç»´åº¦
            dropout=0.1,
            activation='gelu',
            batch_first=True,
            norm_first=True  # Pre-LNï¼Œæ›´ç¨³å®š
        )
        
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=self.num_attn_layers  # ğŸ”¥ 3 å±‚
        )
        
        # ==========================================
        # 3. èåˆåçš„ FFN
        # ==========================================
        self.fusion_mlp = nn.Sequential(
            nn.Linear(self.embed_dim, 128),
            activation,
            nn.Dropout(0.1),
            nn.Linear(128, 64)
        )
        
        # ==========================================
        # 4. GRU å±‚
        # ==========================================
        self.rnn = nn.GRU(
            input_size=64, 
            hidden_size=768, 
            num_layers=2,
            batch_first=True,
            dropout=0.1
        )
        
        # ==========================================
        # 5. è¾“å‡º MLP
        # ==========================================
        self.output_mlp = nn.Sequential(
            nn.Linear(768, 256),
            activation,
            nn.Linear(256, 128),
            activation,
            nn.Linear(128, 32+2),
            last_activation
        )
        self.hidden_states = None
        
        # ==========================================
        # ğŸ”¥ è®¡ç®—å„éƒ¨åˆ†å‚æ•°é‡
        # ==========================================
        proprio_params = sum(p.numel() for p in self.proprio_encoder.parameters())
        attn_params = sum(p.numel() for p in self.transformer_encoder.parameters())
        fusion_params = sum(p.numel() for p in self.fusion_mlp.parameters())
        gru_params = sum(p.numel() for p in self.rnn.parameters())
        output_params = sum(p.numel() for p in self.output_mlp.parameters())
        total_params = sum(p.numel() for p in self.parameters())
        
        print(f"âœ… Recurrent Depth Backbone with 3-Layer Attention initialized:")
        print(f"   ==========================================")
        print(f"   æ¨¡å—åˆ†è§£:")
        print(f"   - Proprio Encoder:     {proprio_params:,} ({proprio_params/1e3:.1f}K)")
        print(f"   - ğŸ”¥ Transformer (3å±‚): {attn_params:,} ({attn_params/1e3:.1f}K)")
        print(f"     * Attention:         {self.num_heads} heads Ã— {self.num_attn_layers} layers")
        print(f"     * Embed dim:         {self.embed_dim}")
        print(f"     * FFN hidden:        128")
        print(f"   - Fusion MLP:          {fusion_params:,} ({fusion_params/1e3:.1f}K)")
        print(f"   - GRU (2å±‚):           {gru_params:,} ({gru_params/1e3:.1f}K)")
        print(f"   - Output MLP:          {output_params:,} ({output_params/1e3:.1f}K)")
        print(f"   ==========================================")
        print(f"   æ€»å‚æ•°é‡:              {total_params:,} (~{total_params/1e6:.2f}M)")
        print(f"   ä¼°è®¡æ˜¾å­˜å ç”¨:          ~{total_params * 4 / 1024**2 * 2:.1f} MB (FP32)")

    def forward(self, depth_image, proprioception):
        batch_size = depth_image.shape[0]
        
        # ==========================================
        # 1. ç¼–ç  Depth å’Œ Proprioception
        # ==========================================
        depth_latent = self.base_backbone(depth_image)  # [batch, 32]
        proprio_latent = self.proprio_encoder(proprioception)  # [batch, 32]
        
        # ==========================================
        # 2. æ‹¼æ¥æˆåºåˆ—ï¼ˆä½œä¸º Transformer çš„è¾“å…¥ï¼‰
        # ==========================================
        # å°† depth å’Œ proprio ä½œä¸ºä¸¤ä¸ª token
        tokens = torch.stack([depth_latent, proprio_latent], dim=1)  # [batch, 2, 32]
        
        # ==========================================
        # 3. ğŸ”¥ 3 å±‚ Transformer Encoder
        # ==========================================
        # æ¯å±‚åŒ…å«: Multi-Head Attention + FFN + Residual + LayerNorm
        attn_out = self.transformer_encoder(tokens)  # [batch, 2, 32]
        
        # ==========================================
        # 4. èåˆç‰¹å¾
        # ==========================================
        # å–å¹³å‡æ± åŒ–ï¼ˆå°†ä¸¤ä¸ª token èåˆï¼‰
        fused_latent = attn_out.mean(dim=1)  # [batch, 32]
        
        # é€šè¿‡ FFN
        ffn_out = self.fusion_mlp(fused_latent)  # [batch, 64]
        
        # ==========================================
        # 5. GRU å¤„ç†
        # ==========================================
        depth_latent, self.hidden_states = self.rnn(
            ffn_out[:, None, :], 
            self.hidden_states
        )  # [batch, 1, 768]
        
        # ==========================================
        # 6. è¾“å‡º MLP
        # ==========================================
        depth_latent = self.output_mlp(depth_latent.squeeze(1))  # [batch, 34]
        
        return depth_latent

    def detach_hidden_states(self):
        if self.hidden_states is not None:
            self.hidden_states = self.hidden_states.detach().clone()

class RecurrentDepthBackbone_GRU(nn.Module):
    def __init__(self, base_backbone, env_cfg) -> None:
        super().__init__()
        activation = nn.ELU()
        last_activation = nn.Tanh()
        self.base_backbone = base_backbone
        
        if env_cfg == None:
            # åŠ æ·±ç½‘ç»œæ·±åº¦ï¼š3å±‚ MLP
            self.combination_mlp = nn.Sequential(
                                    nn.Linear(32 + 53, 256),
                                    activation,
                                    nn.Linear(256, 128),
                                    activation,
                                    nn.Linear(128, 64)
                                )
        else:
            # åŠ æ·±ç½‘ç»œæ·±åº¦ï¼š3å±‚ MLP
            self.combination_mlp = nn.Sequential(
                                        nn.Linear(32 + env_cfg.env.n_proprio, 256),
                                        activation,
                                        nn.Linear(256, 128),
                                        activation,
                                        nn.Linear(128, 64)
                                    )
        
        # åŠ æ·± GRUï¼š2å±‚ï¼Œå¢åŠ éšè—å±‚ç»´åº¦åˆ° 768
        self.rnn = nn.GRU(
            input_size=64, 
            hidden_size=768, 
            num_layers=2,      # ğŸ”¥ ä» 1 å±‚å¢åŠ åˆ° 2 å±‚
            batch_first=True,
            dropout=0.1        # æ·»åŠ  dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        
        # åŠ æ·±è¾“å‡º MLPï¼š3å±‚
        self.output_mlp = nn.Sequential(
                                nn.Linear(768, 256),
                                activation,
                                nn.Linear(256, 128),
                                activation,
                                nn.Linear(128, 32+2),
                                last_activation
                            )
        self.hidden_states = None

    def forward(self, depth_image, proprioception):
        depth_image = self.base_backbone(depth_image)  # [batch, 32]
        depth_latent = self.combination_mlp(torch.cat((depth_image, proprioception), dim=-1))  # [batch, 64]
        
        # RNN å¤„ç†
        depth_latent, self.hidden_states = self.rnn(depth_latent[:, None, :], self.hidden_states)  # [batch, 1, 768]
        depth_latent = self.output_mlp(depth_latent.squeeze(1))  # [batch, 34]
        
        return depth_latent

    def detach_hidden_states(self):
        if self.hidden_states is not None:
            self.hidden_states = self.hidden_states.detach().clone()


import torch
import torch.nn as nn
import math

class DepthTransformerBackbone(nn.Module):
    """è½»é‡çº§å•é€šé“æ·±åº¦å›¾ Transformerï¼ˆæ˜¾å­˜å‹å¥½ï¼‰"""
    
    def __init__(self, prop_dim, scandots_output_dim, hidden_state_dim, output_activation=None, num_frames=1):
        super().__init__()
        
        self.scandots_output_dim = scandots_output_dim
        activation = nn.ELU()
        
        # ==========================================
        # ğŸ”¥ è½»é‡çº§è¶…å‚æ•°ï¼ˆå¤§å¹…å‡å°‘å‚æ•°é‡ï¼‰
        # ==========================================
        self.patch_size = 16         # ğŸ”¥ ä» 8 å¢åŠ åˆ° 16 (å‡å°‘ patch æ•°é‡)
        self.embed_dim = 128         # ğŸ”¥ ä» 256 å‡å°‘åˆ° 128 (å‡å°‘éšè—ç»´åº¦)
        self.num_heads = 4           # ğŸ”¥ ä» 8 å‡å°‘åˆ° 4 (å‡å°‘æ³¨æ„åŠ›å¤´)
        self.num_layers = 3          # ğŸ”¥ ä» 6 å‡å°‘åˆ° 3 (å‡å°‘å±‚æ•°)
        self.mlp_ratio = 2           # ğŸ”¥ ä» 4 å‡å°‘åˆ° 2 (FFN æ›´å°)
        
        # è®¡ç®— patch æ•°é‡
        self.num_patches_h = 58 // self.patch_size  # 3
        self.num_patches_w = 87 // self.patch_size  # 5
        self.num_patches = self.num_patches_h * self.num_patches_w  # 15 (åŸæ¥ 70)
        
        # ==========================================
        # 1. Patch Embeddingï¼ˆå•é€šé“å·ç§¯ï¼‰
        # ==========================================
        self.patch_embed = nn.Conv2d(
            in_channels=1,
            out_channels=self.embed_dim,
            kernel_size=self.patch_size,
            stride=self.patch_size,
            padding=0
        )
        
        # ==========================================
        # 2. CLS Token å’Œ Position Embedding
        # ==========================================
        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.embed_dim))
        
        # åˆå§‹åŒ–
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        
        # ==========================================
        # 3. Transformer Encoderï¼ˆæ›´è½»é‡ï¼‰
        # ==========================================
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.embed_dim,
            nhead=self.num_heads,
            dim_feedforward=self.embed_dim * self.mlp_ratio,  # ğŸ”¥ 256 (åŸæ¥ 1024)
            dropout=0.1,
            activation='gelu',
            batch_first=True,
            norm_first=True  # ğŸ”¥ Pre-LN æ›´ç¨³å®šï¼Œæ”¶æ•›æ›´å¿«
        )
        
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=self.num_layers
        )
        
        # ==========================================
        # 4. è¾“å‡º MLPï¼ˆæ›´ç®€å•ï¼‰
        # ==========================================
        self.output_mlp = nn.Sequential(
            nn.LayerNorm(self.embed_dim),
            nn.Linear(self.embed_dim, 128),  # ğŸ”¥ ä» 256 å‡å°‘åˆ° 128
            activation,
            nn.Dropout(0.1),
            nn.Linear(128, scandots_output_dim)
        )
        
        if output_activation == "tanh":
            self.output_activation = nn.Tanh()
        else:
            self.output_activation = activation
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        
        print(f"âœ… Lightweight Depth Transformer initialized:")
        print(f"   - Input size: [58, 87]")
        print(f"   - Patch size: {self.patch_size}x{self.patch_size}")
        print(f"   - Num patches: {self.num_patches} (å‡å°‘ {100 - self.num_patches/70*100:.1f}%)")
        print(f"   - Embed dim: {self.embed_dim} (å‡å°‘ 50%)")
        print(f"   - Num layers: {self.num_layers} (å‡å°‘ 50%)")
        print(f"   - Num heads: {self.num_heads} (å‡å°‘ 50%)")
        print(f"   - FFN hidden: {self.embed_dim * self.mlp_ratio} (å‡å°‘ 75%)")
        print(f"   - Output dim: {scandots_output_dim}")
        print(f"   - Total parameters: {total_params:,} (~{total_params/1e6:.2f}M)")
        print(f"   - ä¼°è®¡æ˜¾å­˜å ç”¨: ~{total_params * 4 / 1024**2 * 2:.1f} MB (FP32)")

    def forward(self, images: torch.Tensor):
        """
        è¾“å…¥: images [batch, 58, 87] æ·±åº¦å›¾
        è¾“å‡º: latent [batch, scandots_output_dim] (32 ç»´)
        """
        batch_size = images.shape[0]
        
        # ==========================================
        # 1. Patch Embedding
        # ==========================================
        x = images.unsqueeze(1)  # [batch, 1, 58, 87]
        x = self.patch_embed(x)  # [batch, 128, 3, 5]
        x = x.flatten(2).transpose(1, 2)  # [batch, 15, 128]
        
        # ==========================================
        # 2. æ·»åŠ  CLS Token
        # ==========================================
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # [batch, 1, 128]
        x = torch.cat([cls_tokens, x], dim=1)  # [batch, 16, 128]
        
        # ==========================================
        # 3. æ·»åŠ  Position Embedding
        # ==========================================
        x = x + self.pos_embed  # [batch, 16, 128]
        
        # ==========================================
        # 4. Transformer Encoder
        # ==========================================
        x = self.transformer(x)  # [batch, 16, 128]
        
        # ==========================================
        # 5. å– CLS Token ä½œä¸ºå…¨å±€ç‰¹å¾
        # ==========================================
        cls_output = x[:, 0]  # [batch, 128]
        
        # ==========================================
        # 6. MLP æ˜ å°„åˆ°ç›®æ ‡ç»´åº¦
        # ==========================================
        latent = self.output_mlp(cls_output)  # [batch, 32]
        latent = self.output_activation(latent)
        
        return latent

# ==========================================
# ğŸ”¥ åŸå§‹ç‰ˆæœ¬ï¼šè½»é‡çº§ CNN
# ==========================================
class RecurrentDepthBackbone_Original(nn.Module):
    """
    åŸå§‹ç‰ˆæœ¬ï¼šä½¿ç”¨æµ…å±‚ç½‘ç»œå’Œå•å±‚ GRU
    å¦‚éœ€ä½¿ç”¨æ­¤ç‰ˆæœ¬ï¼Œè¯·ä¿®æ”¹é…ç½®æ–‡ä»¶
    """
    def __init__(self, base_backbone, env_cfg) -> None:
        super().__init__()
        activation = nn.ELU()
        last_activation = nn.Tanh()
        self.base_backbone = base_backbone
        if env_cfg == None:
            self.combination_mlp = nn.Sequential(
                                    nn.Linear(32 + 53, 128),
                                    activation,
                                    nn.Linear(128, 32)
                                )
        else:
            self.combination_mlp = nn.Sequential(
                                        nn.Linear(32 + env_cfg.env.n_proprio, 128),
                                        activation,
                                        nn.Linear(128, 32)
                                    )
        self.rnn = nn.GRU(input_size=32, hidden_size=512, batch_first=True)
        self.output_mlp = nn.Sequential(
                                nn.Linear(512, 32+2),
                                last_activation
                            )
        self.hidden_states = None

    def forward(self, depth_image, proprioception):
        depth_image = self.base_backbone(depth_image)
        depth_latent = self.combination_mlp(torch.cat((depth_image, proprioception), dim=-1))
        depth_latent, self.hidden_states = self.rnn(depth_latent[:, None, :], self.hidden_states)
        depth_latent = self.output_mlp(depth_latent.squeeze(1))
        
        return depth_latent

    def detach_hidden_states(self):
        if self.hidden_states is not None:
            self.hidden_states = self.hidden_states.detach().clone()


class DepthOnlyFCBackbone58x87_Original(nn.Module):
    """
    åŸå§‹ç‰ˆæœ¬ï¼šä½¿ç”¨ CNN (Conv2d + MaxPool)
    å¦‚éœ€ä½¿ç”¨æ­¤ç‰ˆæœ¬ï¼Œè¯·ä¿®æ”¹é…ç½®æ–‡ä»¶
    """
    def __init__(self, prop_dim, scandots_output_dim, hidden_state_dim, output_activation=None, num_frames=1):
        super().__init__()

        self.num_frames = num_frames
        activation = nn.ELU()
        self.image_compression = nn.Sequential(
            # [1, 58, 87]
            nn.Conv2d(in_channels=self.num_frames, out_channels=32, kernel_size=5),
            # [32, 54, 83]
            nn.MaxPool2d(kernel_size=2, stride=2),
            # [32, 27, 41]
            activation,
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),
            activation,
            nn.Flatten(),
            # [32, 25, 39]
            nn.Linear(64 * 25 * 39, 128),
            activation,
            nn.Linear(128, scandots_output_dim)
        )

        if output_activation == "tanh":
            self.output_activation = nn.Tanh()
        else:
            self.output_activation = activation

    def forward(self, images: torch.Tensor):
        images_compressed = self.image_compression(images.unsqueeze(1))
        latent = self.output_activation(images_compressed)

        return latent

# ä½¿ç”¨åˆ«åä¿æŒå…¼å®¹æ€§
RecurrentDepthBackbone = RecurrentDepthBackbone_Attention 
DepthOnlyFCBackbone58x87 = DepthResNetBackbone